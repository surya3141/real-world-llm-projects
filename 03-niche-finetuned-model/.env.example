# Hugging Face Token (required for Llama 3 access)
HF_TOKEN=your_huggingface_token_here

# Weights & Biases (optional - for training monitoring)
WANDB_API_KEY=your_wandb_api_key_here
WANDB_PROJECT=python-api-finetuning

# Model Configuration
MODEL_NAME=meta-llama/Meta-Llama-3-8B
MODEL_CACHE_DIR=./models/cache

# Training Configuration
MAX_SEQ_LENGTH=2048
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=2e-4
NUM_EPOCHS=3

# LoRA Configuration
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.1

# Inference Configuration
LOAD_IN_4BIT=True
USE_FLASH_ATTENTION=True
